Critical UX Gaps to Address
Enhancing the VS Code AI Assistant to Codex/

Copilot-Level UX and Capability

1. Integrate Ghost Text for Inline Suggestions

To mimic the Codex/Copilot user experience, implement inline “ghost text” code completions directly in

the VS Code editor. This means as the user types, the AI assistant should automatically display dim, italicized

suggestion text (ghost text) at the cursor, which the user can accept with a    Tab    press
. VS Code
supports this via the Inline Completion API. You can register an      InlineCompletionItemProvider      in
your extension (as Copilot does) to provide these suggestions in real time
. This approach removes
the extra “propose-then-apply” step – instead of requiring the user to explicitly request and apply an edit,

the assistant continuously offers suggestions as they code, just like GitHub Copilot’s ghost text completions.

Key steps and tips:

- Use VS Code’s Inline Completion Proposal API:    Enable the Inline Completions API (Proposed) in your

extension. The Copilot VS Code extension, for example, declares the "inlineCompletionsAdditions"

proposal and uses registerInlineCompletionItemProvider to supply ghost text suggestions
.

-    Tab-to-Accept Workflow:    Ensure that when a suggestion is shown, pressing    Tab    inserts the suggested

code. This matches the intuitive workflow developers expect

. Partial acceptance (e.g. accepting word-by-
word with a shortcut) is a nice-to-have feature that Copilot supports

, but initial focus should be on full

suggestion acceptance with Tab.

- Match Coding Style:    Aim for the suggestions to respect the project’s existing coding style (naming

conventions, formatting). Copilot automatically mirrors the style it infers from the open file
. You can

encourage this by including relevant code context (more on context below) and possibly project style

guidelines in the prompt so the model’s completions blend in naturally.

By integrating ghost text suggestions, you bring the assistant’s UX on par with Copilot – real-time,

unobtrusive suggestions that feel like an AI pair-programmer working alongside the user
. This

dramatically improves the developer experience by reducing friction (the user doesn’t have to issue a

separate “propose” command for every completion).
2. Inline Completions (Ghost Text)
Missing: As-you-type suggestions Add: Lightweight completion endpoint
python
@router.post("/{workspace_id}/vscode/agent/complete")
async def inline_complete(
        file_path: str,
        prefix: str,    # Code before cursor
        suffix: str,    # Code after cursor
        cursor_position: dict,
        ...
) -> dict:
        # Use faster model (Claude Haiku) for speed
        # Return: {"completion": "...", "range": {...}}
Key: Must respond in <100ms. Cache file context, use smaller models.

2. Real-Time & Continuous Code Completions

Codex/Copilot-level performance means the assistant should offer real-time, continuous completions as

the user types. Instead of waiting for an explicit user prompt, the extension can listen for pauses or certain

trigger characters (e.g. a new line or the end of a function signature) and then send the current context to

the backend for completion. Some strategies to achieve this:

Trigger on Idle/Delimiter: Detect when the user has paused typing for a few hundred milliseconds

or typed a trigger (like “/” to start a comment, a newline, or an opening bracket) and call the

completion API. This simulates how Copilot automatically offers a suggestion whenever it recognizes

a cue (like a function docstring or comment)
.

Streaming Responses: If using the GPT API’s streaming capability, stream tokens to the extension

so the ghost text can appear character by character. This makes the AI feel faster and more fluid.

Even if the full suggestion takes a moment to generate, streaming partial output can let the user see

the suggestion form in real-time (Copilot does this under the hood to minimize perceived latency).

Short Prompts for Speed: For quick inline completions, consider sending a more focused prompt

(e.g. just the current file and maybe a summary of open files) to a faster model. Copilot likely uses a

relatively high-performance model for ghost text suggestions. If you have access to different model

sizes or versions (e.g. GPT-3.5 vs GPT-4), you could use the faster model for on-the-fly suggestions

and reserve the more powerful model for explicit “ask the assistant” queries or refactoring tasks. In

fact, VS Code now even allows switching the AI model for suggestions – you could expose a

similar option if using multiple models, so users can trade off latency vs. accuracy.

Next-Edit Suggestions: Beyond completing the current line or block, Copilot now has Next Edit

Suggestions (NES) which predict the next likely edit you’ll make
. Your backend already generates

structured edit plans (with a “plan” and list of edits
); you can integrate this by having the

extension highlight or cue those predicted edits. For example, if the AI foresees the user will need to

add an import or modify another function, it could present that as a subtle suggestion or a lightbulb

action. This anticipates multi-step code changes and makes the AI feel more proactive. (Copilot NES

monitors the edits you're making and offers to apply subsequent changes automatically
.)

By making completions continuous and low-latency, the assistant will feel as responsive as Copilot/Codex.

The user should get immediate suggestions in-line while typing, which they can accept or ignore without

breaking flow.

3. Offer Multiple Suggestions and User Choices

Codex and Copilot sometimes produce multiple alternative suggestions for a given prompt, allowing the

developer to pick the best one
. To reach that level, consider generating and exposing more than one

completion: for instance, the top 2–3 different ways to implement a function. The VS Code UI can allow

cycling through suggestions or showing a drop-down/hover widget to choose an alternative.

Implementation ideas:

- Use the GPT API’s ability to return multiple completions (if available) or call it multiple times with slight

variations (e.g. different randomness seeds) to gather a few distinct suggestions.

- In the extension, when a suggestion is present, allow the user to hover or press a shortcut (Copilot uses

Ctrl+Enter or ⌘+Enter
) to open a panel of alternatives
. VS Code’s inline suggestion UI supports

this kind of alternative view
.

- Let the user accept part of a suggestion: as mentioned, VS Code/Copilot have a key binding to accept the

next word or line from the ghost text
. Implementing this gives finer control – the user can use the AI’s

suggestion as a starting point and then continue coding themselves.

Providing multiple options can increase user confidence in the AI (they can compare choices) and helps

cover cases where one suggestion might not be perfect. It brings your assistant closer to Copilot’s polish,

where developers can cycle through ideas quickly without context switching to chat.
1. Real-time Streaming & Progressive Edits
Current: Single JSON response with all edits Target: Stream edits as they're generated
python
# Add streaming endpoint
@router.post("/{workspace_id}/vscode/agent/propose-stream")
async def propose_edits_stream(workspace_id: str, ...):
        async def generate():
                async for chunk in llm_stream_edits(...):
                        # Stream partial edits as they're generated
                        yield json.dumps({"type": "edit", "file": ..., "content": ...})
        return StreamingResponse(generate(), media_type="text/event-stream")
Why it matters: Users see progress immediately; feels responsive vs. waiting for entire response.
3. Multi-file Edit Planning
Current: Returns edits for explicit file list Improve: Agent decides which files to modify
python
# Phase 1: Planning
plan = await llm.plan_changes(task, workspace_context)
# Returns: {"files_to_modify": [...], "files_to_read": [...], "reasoning": "..."}


# Phase 2: Gather context for those files
additional_context = await gather_file_contents(plan.files_to_read)


# Phase 3: Generate edits
edits = await llm.generate_edits(plan, additional_context)
4. Diff Preview & Apply Modes
Current: Extension applies newText Add: Multiple application strategies
python
class EditMode(str, Enum):
        REPLACE = "replace"    # Full file replacement
        DIFF = "diff"                # Unified diff with line numbers
        SEARCH_REPLACE = "search_replace"    # Find/replace pairs
        INSERT = "insert"        # Insert at position


@dataclass
class Edit:
        file: str
        mode: EditMode
        original_lines: Optional[List[int]]    # For verification
        content: str
        description: str    # User-facing explanation
UX: Let users preview side-by-side diff before accepting.
5. Semantic Code Context (Not Just Snapshots)
Current: Extension sends bounded snapshots Upgrade: Build codebase index
python
# Background indexer
class CodebaseIndex:
        async def index_workspace(self, workspace_id: str):
                # Index symbols, imports, definitions
                # Store in vector DB (already have RAG)
                await self.index_symbols(...)
                await self.index_imports(...)
                await self.index_docstrings(...)
        
        async def find_relevant_context(
                self, 
                query: str,
                current_files: List[str]
        ) -> List[CodeContext]:
                # Semantic search for relevant code
                # Return: related functions, imports, tests
Why: Agent can pull in relevant context without user selecting files.
4. Enhanced Context via Code Indexing and Semantic Search

One area where you can surpass Codex/Copilot is by leveraging Parallel’s broader knowledge base. Copilot

primarily looks at the open file and a few related files for context
, but your platform can integrate much

richer context. Here’s how to maximize that:

Codebase Indexing: Implement a service to index the entire repository (or at least the workspace)

and use embeddings for semantic code search. GitHub Copilot Chat now automatically indexes your

repo (either remotely on GitHub’s servers or locally) to retrieve relevant code snippets when needed

. You can do similarly by creating a vector index of functions, classes, and comments in the

project. When the user asks a question or the AI is formulating a suggestion, you can perform a

semantic search for relevant code (e.g. find the definition of a function being called, or similar

implementations)
. The retrieved snippets or file references can be added to the prompt as

additional context. This ensures the AI has the relevant parts of the codebase at its fingertips, beyond

just the currently open files. According to Microsoft’s documentation, using advanced vector

embeddings to find semantically similar code and adding it to context greatly improves the

relevance of suggestions
.

Workspace Knowledge Base (RAG): Take advantage of Parallel’s database by doing Retrieval-

Augmented Generation over project documentation, requirements, and prior discussions. For

instance, if the user is working on a task that has a spec or user story in your system, fetch that

description from the DB and supply it to the AI. This grounds the code suggestions in the actual

requirements or design decisions. IBM’s Watsonx Code Assistant (a similar AI coding tool) uses RAG

to pull in both code and documentation context, which reduces hallucinations and improves accuracy

of the generated code
. In your case, this could include design docs, API schemas, or even the

timeline of related tasks in Parallel. By bridging to the Parallel DB, your agent can answer questions

like “What was the decided approach for feature X?” or incorporate relevant business logic details

while coding – something vanilla Copilot cannot do.

Automatic Context Expansion: You might implement logic to decide when to pull in extra context.

For example, if the user’s prompt or code indicates a reference to a module not in the current file,

auto-include that module’s code (via the index). If the user asks a high-level question (“How do I

implement the Foo algorithm we discussed?”), the assistant could retrieve the relevant project note

or previous chat message from the DB and feed it into the LLM. Copilot Chat does this by performing

a semantic search when it “determines it needs more project context”
. Your agent can similarly

decide to query the Parallel knowledge base or code index when the query is broad or unclear.

By blending code indexing with knowledge-base retrieval, you provide a broader and smarter context

than Codex/Copilot. The assistant will be less likely to produce irrelevant or inconsistent code since it can

ground its answers in actual project data. This is a key advantage of your platform – use it to make the AI a

true “project-aware” assistant, not just a code autocompleter.
8. LSP Integration (Diagnostics, Symbols)
Current: Extension sends diagnostics in snapshot Enhance: Deeply integrate LSP data
python
@dataclass
class LSPContext:
        diagnostics: List[Diagnostic]    # Errors, warnings
        symbols: List[Symbol]                    # Functions, classes in file
        references: List[Location]          # Where symbol is used
        hover_info: Optional[str]            # Type info at cursor
        
# Include in prompt
context_prompt = f"""
Current file has {len(lsp.diagnostics)} errors:
{format_diagnostics(lsp.diagnostics)}


You're working on function: {lsp.current_symbol.name}
Type signature: {lsp.hover_info}
"""
9. Smart Context Window Management
Current: Limited by single snapshot Improve: Intelligent context pruning
python
class ContextManager:
        def select_context(
                self,
                user_query: str,
                available_files: List[File],
                max_tokens: int = 100_000
        ) -> List[File]:
                # Priority scoring
                scores = []
                for file in available_files:
                        score = 0
                        score += self.relevance_to_query(file, user_query)
                        score += self.recency_boost(file)
                        score += self.open_file_boost(file)
                        score += self.related_to_error_boost(file)
                        scores.append((file, score))
                
                # Pack files until token limit
                return self.pack_by_score(scores, max_tokens)

6. Incorporate Project Memory and Preferences

One way Copilot is becoming more “project-aware” is through features like Copilot Memories, which learn

and enforce a project’s specific conventions over time
. You can introduce a similar concept to improve

your AI’s long-term usefulness:

Learning from Corrections: When the user edits or rejects the AI’s suggestions, try to capture that

feedback. For example, if the AI consistently suggests a certain code style that the user then

changes, you could adjust the assistant’s behavior (perhaps by updating the system prompt or

context with a note about the preferred style). Copilot’s implementation actually nudges users to

save preferences (like “always use single quotes in JSON” or a naming convention) which then get

stored in files like .editorconfig or CONTRIBUTING.md
. Your assistant could simply notice

these files if they exist and adapt (e.g., if the repo has an .editorconfig
, ensure the suggestions

follow those indent/format rules).

Persisting Context Across Sessions: It sounds like you already store chat history and have an agent

identity per user in the DB. Leverage that to maintain continuity. For instance, if the team agreed on

using a specific library for a task (recorded in a Parallel task description or prior assistant message),

the AI should remember and keep suggesting that library in future coding sessions. Maintaining this

memory (within reasonable context limits) will make the AI feel like it “remembers” project decisions,

much like a human team member. Just be mindful to summarize or distill long histories, so the

prompt doesn’t overflow – this could be a background job that periodically summarizes project chats

or task notes for quick reference.

Guidelines and Agent Files: You might introduce a notion of a project guide file (similar to OpenAI’s

idea of AGENTS.md for their Codex agent
). This file in the repo (or a config in your DB) could

contain high-level instructions for the AI: coding standards, architectural patterns to follow, banned

functions, etc. The assistant would read this (or have it preloaded in context) every time. This ensures

consistency and allows teams to explicitly steer the AI. For example, if AGENTS.md says “All

database queries must go through XYZ API – do not write raw SQL”, the AI would then avoid

suggesting raw SQL in code. This goes beyond Copilot’s current feature set and moves towards

enterprise AI assistant use-cases, but it can be powerful for alignment.

By implementing a form of project memory and preferences, your AI will not only match Copilot’s

capabilities but can exceed them in project-specific reliability. Over time, the assistant feels less like a

generic model and more like it truly understands your project.
6. Agentic Tool Use (Not Just Single Shot)
Current: Single LLM call → edits Upgrade: Multi-turn with tools
python
tools = [
        {
                "name": "read_file",
                "description": "Read additional file contents",
                "parameters": {"file_path": str}
        },
        {
                "name": "search_code",
                "description": "Search codebase for pattern",
                "parameters": {"query": str, "file_pattern": str}
        },
        {
                "name": "run_command",
                "description": "Execute terminal command",
                "parameters": {"command": str}
        },
        {
                "name": "apply_edit",
                "description": "Apply code change",
                "parameters": {"file": str, "edit": str}
        }
]


# Agent loop
while not done:
        response = await llm.complete(messages, tools=tools)
        if response.tool_calls:
                for tool_call in response.tool_calls:
                        result = await execute_tool(tool_call)
                        messages.append({"role": "tool", "result": result})
        else:
                done = True
7. Terminal Integration
Missing: Can't see/use terminal output Add: Terminal context gathering
python
@router.post("/{workspace_id}/vscode/terminal/output")
async def capture_terminal_output(
        command: str,
        output: str,
        exit_code: int,
        cwd: str,
        ...
):
        # Store recent terminal activity
        # Agent can reference in next propose call

8. Utilize AI Commands and Automation (Beyond Copilot)

One unique aspect of your backend design is the ability for the AI to propose commands (e.g., run tests,

perform searches). While Copilot doesn’t currently run commands on behalf of the user, you have an

opportunity to implement an “AI Agent mode” that goes further – somewhat akin to the experimental

OpenAI Codex agent. For example, OpenAI’s Codex (2025) can autonomously run test suites, linters, etc., in

a sandbox as it works on a task
. You could introduce a safer, user-controlled version of this:

On-Demand Agent Execution: Provide a command like “AI: Complete this task” which, when

invoked, allows the assistant to iterate through the plan-execute cycle. Since your model already

outputs a plan and potential commands
, the extension could prompt the user for approval to

execute those commands. For instance, if the model suggests running npm test or a search

query, the extension can ask “The AI wants to run tests to verify its changes – allow this?” If

approved, run the tests (perhaps in a sandbox or read-only mode first) and feed the results back to

the AI for analysis. This closed-loop can continue for a few iterations (with user oversight) until the AI

either succeeds or needs guidance. This approach can help achieve more complex refactorings or

bug fixes automatically, beyond single-step edits. It’s basically bringing some of the power of

OpenAI’s Codex agent (which can handle multi-step tasks and even commit changes after tests pass

) into your VS Code assistant.

Task/Goal Integration: Because Parallel’s platform has a notion of tasks in the workspace, you

could tie the agent mode to tasks. For example, a user could click “Implement this task with AI” and

the agent will try to write the code for that task, referencing the task description and then possibly

running tests or analysis to validate. This is a level of automation beyond Copilot – more like an AI

junior developer that attempts to complete a ticket. It sounds futuristic, but your architecture (with

structured plan/edits and secure scopes) is partway there. By gating each step with scopes and user

permission, you maintain safety (the AI can’t do anything truly harmful without consent)
.

Safety and Verification: If you go down this route, continue to emphasize transparency. For any

automated change or command, show the user the outcome (test results, diff of code changes, etc.)

for verification. OpenAI’s Codex provides “verifiable evidence of its actions through logs and test

outputs”, and explicitly communicates failures or uncertainties
. Your assistant should

similarly report what it did and any issues encountered, so the user stays in control. This builds trust

and ensures the human is always the final gate for code going into the codebase
.

While this kind of autonomous coding agent is beyond Copilot’s current feature set, it could be a

differentiator for Parallel’s assistant. Even starting with simple automations (like “AI fix all lint errors in this

file” where it might run a lint command, get output, and apply fixes) can elevate the experience. Just

introduce these capabilities gradually and keep the user in the loop to avoid overwhelming or surprising

them.

5. Optimize Model Selection and Prompting for Code Tasks

Since you are using the GPT API, ensure you leverage it in a way that best mimics Codex/Copilot’s strengths

in coding. A few suggestions:

Use Code-Specialized Models: If available, use models finetuned for code. OpenAI Codex itself was

a GPT-3 model further trained on source code, which made it especially good at understanding code

structure and producing syntactically correct output
. In 2026, models like GPT-4 (with Code

Interpreter) or GPT-3.5 Turbo (code-tuned versions) might be options. Choose a model that excels at

completion-style prompts and can handle the syntax of the languages in your project. If using

OpenAI’s chat models, make sure to set the system message with instructions to focus on code and

be concise when appropriate (to avoid overly verbose explanations unless asked).

Temperature and Determinism: Codex and Copilot are tuned to be relatively deterministic to avoid

random or unsafe code. Set a low temperature (e.g. 0 – 0.2) for completion requests so that the

suggestions are stable and repeatable, especially for inline completions. This will yield more

predictable ghost text (Copilot’s suggestions feel consistent because they don’t wildly change on

each request for the same context). You might allow a slightly higher temperature for brainstorming

or chat mode (when the user asks for alternative approaches or creative solutions), but for the core

coding assistance keep it focused.

Few-Shot Prompting or Schemas: You already use a JSON schema for the propose endpoint, which

is good for structured responses. Consider maintaining a library of few-shot examples or

instructions that nudge the model toward better coding practices. For instance, “When generating

code, ensure it follows the project’s coding style and only change relevant parts of the file” – this can

be part of the system or developer message. This is akin to OpenAI’s Codex instructing the model to

produce “clean patches ready for immediate review”
. In fact, OpenAI’s own codex-1 model was

trained via RLHF to align with human coding preferences for clean, minimal diffs
. While you may

not train your own model, you can still guide GPT output through prompt engineering to get lean,

high-quality diffs (e.g., ask for just the code changes without extra commentary in the edits

output).

Model Switching Options: As a more advanced feature, you could let users choose between models

(for example, a faster vs a smarter model) similar to Copilot’s “Change Completions Model”

command
. This could be as simple as a config setting to use GPT-4 or GPT-3.5. It’s not strictly

necessary, but power users might appreciate it, and it signals that your tool can utilize the latest

models as they evolve.

Overall, treat the model as configurable engine and tune it for coding tasks: ensure it’s fed the right

context (as per point 4) and guided to output code in the desired format. This will bring the behavior closer

to Codex’s focused code-generation style.

7. Improve Chat and Code Review Integration

Your platform already supports a chat mode ( POST /api/v1/vscode/chat ) with repository context. To

make it more Copilot-like (especially Copilot X’s chat), consider tighter integration between chat and the

editor:

In-Editor Chat Links: Allow the user to highlight code in VS Code and ask the AI questions or for

modifications in a chat sidebar. Copilot X Chat does this by providing an inline action to “Explain this

code” or “Refactor this code” which opens the chat with the selected code snippet in context. You can

implement a context menu or command palette action in the extension that sends the selected text

to your chat endpoint, so the assistant can act on it. This way, the user doesn’t have to copy-paste

code into chat manually – it’s seamless.

Refinement Loop: If the AI’s inline suggestion isn’t exactly right, make it easy to refine via chat. For

example, the user could trigger a “Why this suggestion?” or “Improve this further” prompt that sends

the ghost suggestion and context into the chat channel, letting the AI elaborate or adjust. This

combines the benefits of ghost text (quick suggestions) with the depth of chat (explanations and

iterative refinement).

Code Review and Explanations: You could introduce commands for the AI to review code (either a

diff or an open file) and comment on it, akin to a code review assistant. Copilot doesn’t exactly do full

code reviews out-of-the-box, but tools like OpenAI’s Codex (in ChatGPT) can analyze diffs and explain

changes
. Since your backend can already generate a plan and edits
, you might present

that plan to the user in chat: e.g., “Here’s my plan: 1) Add function X in file Y, 2) Modify Z in file A… Do

you want to proceed?” – and the user can confirm or tweak it via conversation. This makes the

editing process more transparent and interactive, which is great for user trust and understanding. It

appears more like a pair-programmer discussing changes, rather than a magic black box.

Multi-File Awareness in Chat: Ensure that your chat mode also benefits from the code indexing and

knowledge base (discussed in section 4). Copilot Chat will automatically pull in relevant file context

when you ask it something, even if you didn’t manually provide those files
. Your chat should do

the same: if the user asks a high-level question (“How does this library function work?”), retrieve the

source from the code index or docs from the DB and have the AI cite or summarize it. This can be

cited similarly to how your platform already handles timeline or documentation references.

By refining the chat experience in these ways, the VS Code extension becomes more than just an

autocomplete – it turns into a full coding assistant. This aligns with the trajectory of Copilot X, where the

chat and inline suggestions converge into a holistic assistant experience
. Your tool will not only suggest

code like Copilot but also discuss and explain code like ChatGPT, all within the editor.
10. Response Caching
python
from functools import lru_cache
import hashlib


def cache_key(workspace_id: str, files: List[str], query: str) -> str:
        content_hash = hashlib.sha256(
                f"{workspace_id}:{sorted(files)}:{query}".encode()
        ).hexdigest()
        return f"propose:{content_hash}"


# Cache completed responses
async def propose_with_cache(...):
        key = cache_key(...)
        cached = await redis.get(key)
        if cached:
                return json.loads(cached)
        
        result = await propose_edits(...)
        await redis.setex(key, 3600, json.dumps(result))
        return result
11. Optimistic UI Updates
Frontend pattern: Show edits immediately, rollback if fails
typescript
// Extension side
async function applyEdit(edit: Edit) {
    // 1. Apply optimistically
    const backup = document.getText();
    await editor.edit(edit);
    
    try {
        // 2. Verify with backend
        await api.verifyEdit(edit);
    } catch (error) {
        // 3. Rollback on failure
        await editor.edit({ text: backup });
        showError(error);
    }
}
12. Undo Stack Integration
python
# Track edit history per workspace
@dataclass
class EditHistory:
        id: str
        timestamp: datetime
        files_modified: List[str]
        original_content: Dict[str, str]
        new_content: Dict[str, str]
        description: str


@router.post("/{workspace_id}/vscode/agent/undo")
async def undo_last_edit(workspace_id: str, edit_id: str):
        # Restore previous state
        history = await get_edit_history(workspace_id, edit_id)
        return {"files": history.original_content}
13. Test Generation
python
@router.post("/{workspace_id}/vscode/agent/generate-tests")
async def generate_tests(
        file_path: str,
        function_name: str,
        test_framework: str = "pytest"
):
        # Generate unit tests for function
        # Place in appropriate test file
14. Code Explanation & Documentation
python
@router.post("/{workspace_id}/vscode/agent/explain")
async def explain_code(
        code: str,
        cursor_position: Optional[dict] = None
):
        # Return: explanation, docs, similar patterns
15. Git Integration
python
@dataclass
class GitContext:
        current_branch: str
        staged_files: List[str]
        unstaged_changes: List[str]
        recent_commits: List[Commit]


# Agent can:
# - Generate commit messages
# - Explain diffs
# - Suggest PR descriptions

9. Maintain Robust Guardrails and Security

As you enhance capabilities, preserve the strong guardrails you’ve built (and even extend them) to

maintain user trust and project safety. One reason enterprises might prefer your solution over Copilot is the

control and security it offers. Here are some guardrail considerations in the context of Codex-level features:

Scope Enforcement: Continue to enforce OAuth scopes for file access, edits, and command

execution on every request (as you do now). This is a great design because it ensures the AI can only

act within the permissions the user or org has granted. Codex/Copilot don’t expose such fine-

grained controls to end-users, so this can be a selling point for Parallel (especially for sensitive

codebases). Make sure that as you add features like indexing or agent commands, you map them to

appropriate scopes (e.g., a scope for “allow running tests” or “allow repository indexing”).

Path and Content Safety: Your backend normalizes edit paths and restricts edits to allowed files

【vscode.py logic】 – keep that in place. You might also add scanning of AI outputs for disallowed

content. For instance, GitHub Copilot implemented “filtering” of suggestions that are either likely

insecure or match large chunks of public code verbatim (to avoid licensing issues)
. Consider a

similar approach: if the AI suggests something that looks like a secret, credential, or a big copy-

pasted blob, you can intercept and sanitize or warn. Integrating a simple static analysis or using

OpenAI’s content filter on outputs can help flag problematic suggestions before they reach the user.

User Confirmation for Destructive Actions: If your agent mode can run commands or make multi-

file changes, always get user confirmation before applying wide-reaching effects. This is analogous

to how VS Code’s built-in refactoring or search/replace shows a preview of changes. You already

return unified diffs for edits – presenting those diffs in a preview panel for review before the user

clicks “Apply All” could prevent unwanted changes and gives a final safety net.

By reinforcing these guardrails, you ensure that increased power doesn’t come at the cost of safety.

Developers and teams will be more inclined to use an AI assistant that is transparent and secure by

design, which your approach already emphasizes (with its explicit checks and logs in deps.py and

verify_vscode_api.py
). In short, match Copilot’s convenience while exceeding its safety – that

combination will make your tool stand out.

10. Conclusion: Toward a Copilot-Like (or Better) Assistant

In summary, achieving a Codex/Copilot-level AI coding assistant in VS Code involves improvements on

multiple fronts:

UX Enhancements: Adopt ghost text inline completions, real-time suggestions, and easy acceptance

to make the AI feel like a natural part of the editor
. Provide multiple suggestion options and

support partial accept to give users flexibility
.

Context and Intelligence: Expand the assistant’s knowledge by indexing the codebase and

integrating your Parallel database for context
. This will ground the AI’s responses in real

project information, increasing relevance and reducing hallucination.

Model and Quality: Leverage the best available models (or combination of models) for coding tasks,

tune them via prompts for minimal and correct code output, and possibly allow user choice of model

for speed vs accuracy
. Continuously refine the prompt strategy based on user feedback and

project conventions
.

Integrated Tools: Marry the chat and coding experiences – let the AI not only complete code, but

also explain, refactor, and review code within the editor context, much like Copilot Chat but tailored

to your platform’s features. Use the assistant’s ability to plan and run commands to go beyond

passive suggestions, enabling automated fixes or executions with user oversight
.

Safety and Control: Keep the rigorous scope and safety checks in place (and extend them as

needed) so that all these powerful features operate within bounds that the user and organization are

comfortable with. This includes path whitelisting, content filtering, and manual confirmation for

significant actions
.

By implementing these suggestions, your VS Code extension’s AI agent will closely emulate the fluid,

helpful UX of GitHub Copilot/Codex. Developers will get instant, context-aware code completions and the

ability to interact with the AI naturally as they code. Moreover, thanks to Parallel’s unique context

integration and your focus on guardrails, your assistant could surpass Copilot in project-awareness and

safety, delivering a truly next-level AI pair programming experience. Each improvement – from UI polish to

backend intelligence – will bring you closer to the goal of an AI coding assistant that feels as indispensable

as Copilot, with the added superpower of your platform’s knowledge integration.
